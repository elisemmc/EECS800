{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains features in a very wide range. Best way to proceed will be to normalize the dataset. After the data is loaded and normalized, include the intercept term in the dataframe. Use standard deviation for normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = #Load and preprocess the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. After you've added the intercept term, define X as the features in the dataframe. Define Y as the target variable.\n",
    "2. Convert them to a numpy array and define beta(the coeffecients) with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = \n",
    "Y = \n",
    "X = np.matrix(X.values)  \n",
    "Y = np.matrix(Y.values)  \n",
    "beta = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we now have every module to calculate our cost function, we'll go ahead and define it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def costFunction(X, Y, beta):\n",
    "    '''\n",
    "    Compute the Least Square Cost Function.\n",
    "    Return the calculated cost function.\n",
    "    '''\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Define a Gradient Descent method that will update beta in every iteration and also update the cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradientDescent(X, Y, beta, alpha, iters):\n",
    "    '''\n",
    "    Compute the gradient descent function.\n",
    "    Return beta and the cost array.\n",
    "    '''\n",
    "    \n",
    "    return beta, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define alpha and number of iterations of your choice and use them to call to gradientDescent function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#please try different values to see the results, but alpha=0.01 and iters=1000 are suggested.\n",
    "alpha = #Define  \n",
    "iters = #Define\n",
    "\n",
    "result = gradientDescent(X, Y, beta, alpha, iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the Ridge Regression regularization and report the change in coeffecients of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradientDescentRidge(X, Y, beta, alpha, itersreg, ridgeLambda):\n",
    "    '''\n",
    "    Compute the gradient descent function.\n",
    "    Return beta and the cost array.\n",
    "    '''\n",
    "    return beta, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define alpha, number of iterations and lambda of your choice that minimizes the cost function and use them to call to gradientDescent function. Plot the cost graph with iterations titled \"Error vs training\" with and without regularization(y axis labeled as cost and x axix labeled as iterations). Then, calculate the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MSE(beta):\n",
    "    '''\n",
    "    Compute and return the MSE.\n",
    "    '''\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Try differnt values, but the same values for alpha and itersreg are suggested, 0.05 for lambda is suggested\n",
    "alpha = #Define\n",
    "itersreg = #Define\n",
    "ridgeLambda = #Define\n",
    "regResult = gradientDescentRidge(X, Y, beta, alpha, itersreg, ridgeLambda)\n",
    "print regResult[0]\n",
    "\n",
    "#MSE for beta with regularization\n",
    "beta= #Define\n",
    "MSE(beta)\n",
    "\n",
    "\n",
    "#The final result wanted for this portion of the lab is the last plot defined earlier, the explanation regarding the \n",
    "#coeffecients of the parameters with Ridge Regression regularization, and the MSE. Please only include the\n",
    "#MSE in the same .txt file as logistic regression results. Also let the print regResult[0] be there, but do NOT include the \n",
    "#outcome for this print in the .txt file. Add the final plot to your report and explain your algorithm, the plot, and \n",
    "#the MSE. Generally, what you did in this portion of the lab. Finally, explain the coeffiencts with regularization in your report -PDF file-. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
